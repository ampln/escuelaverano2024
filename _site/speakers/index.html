<!DOCTYPE html>
<html lang="en">
<!-- Beautiful Jekyll 5.0.0 | Copyright Dean Attali 2020 -->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

  

  <title>Speakers</title>

  
  <meta name="author" content="AMPLN">
  

  <meta name="description" content="Keynotes Tutorials Panels Dr. Sudipta Kar Senior Applied Scientist at Amazon AGI Sudipta Kar is a Senior Applied Scientist at Amazon AGI. He received his Ph.D. in Computer Science from the University of Houston in 2020 under the supervision of Thamar Solorio. His doctoral research focused on creative text analysis....">

  

  

  <link rel="alternate" type="application/rss+xml" title="Mexican NLP Summer School 2024" href="http://localhost:4000/feed.xml">

  

  

  


  
    
      
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">


    
      
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800">


    
  

  
    
      <link rel="stylesheet" href="/assets/css/bootstrap-social.css">
    
      <link rel="stylesheet" href="/assets/css/beautifuljekyll.css">
    
  

  

  
  
  
    
      <link rel="stylesheet" href="/assets/css/index.css">
    
  

  

  
  <meta property="og:site_name" content="Mexican NLP Summer School 2024">
  <meta property="og:title" content="Speakers">
  <meta property="og:description" content="Keynotes Tutorials Panels Dr. Sudipta Kar Senior Applied Scientist at Amazon AGI Sudipta Kar is a Senior Applied Scientist at Amazon AGI. He received his Ph.D. in Computer Science from the University of Houston in 2020 under the supervision of Thamar Solorio. His doctoral research focused on creative text analysis....">

  

  
  <meta property="og:type" content="website">
  <meta property="og:url" content="http://localhost:4000/speakers/">
  <link rel="canonical" href="http://localhost:4000/speakers/">
  

  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:site" content="@AMPLN">
  <meta name="twitter:creator" content="@AMPLN">

  <meta property="twitter:title" content="Speakers">
  <meta property="twitter:description" content="Keynotes Tutorials Panels Dr. Sudipta Kar Senior Applied Scientist at Amazon AGI Sudipta Kar is a Senior Applied Scientist at Amazon AGI. He received his Ph.D. in Computer Science from the University of Houston in 2020 under the supervision of Thamar Solorio. His doctoral research focused on creative text analysis....">

  

  


  

  

</head>


<body>

  


  <nav class="navbar navbar-expand-xl navbar-light fixed-top navbar-custom top-nav-regular"><a class="navbar-brand navbar-brand-logo" href="http://localhost:4000/"><img alt="Mexican NLP Summer School 2024 Logo" src="/assets/images/ampln_header.png"/></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="main-navbar">
    <ul class="navbar-nav ml-auto">
          <li class="nav-item dropdown">
            <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Program information</a>
            <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="/speakers">Speakers</a>
                  <a class="dropdown-item" href="/program">Program</a>
            </div>
          </li>
        
          <li class="nav-item">
            <a class="nav-link" href="/registration">Registration</a>
          </li>
          <li class="nav-item dropdown">
            <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">About the school</a>
            <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="/about">About</a>
                  <a class="dropdown-item" href="/chairs">Organizers</a>
                  <a class="dropdown-item" href="/previousEditions">Previous editions</a>
                  <a class="dropdown-item" href="/CodigoConducta">Code of Conduct</a>
            </div>
          </li>
        
          <li class="nav-item">
            <a class="nav-link" href="/sponsors">Sponsors</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/contact">Contact</a>
          </li></ul>
  </div>

  

  

</nav>


  <!-- TODO this file has become a mess, refactor it -->







<header class="header-section ">

<div class="intro-header no-img">
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="page-heading">
          <h1>Speakers</h1>
          

          
        </div>
      </div>
    </div>
  </div>
</div>
</header>





<div class=" container-md " role="main">
  <div class="row">
    <div class=" col-xl-8 offset-xl-2 col-lg-10 offset-lg-1 ">
      

      <!-- <div class="list-filters">
  <a href="/escuelaverano2024/" class="list-filter">Home</a>
  <a href="/escuelaverano2024/speakers/" class="list-filter filter-selected">Speakers</a>
  <a href="/escuelaverano2024/program/" class="list-filter">Program</a>
  <a href="/escuelaverano2024/about/" class="list-filter">About</a>
</div> -->
<!-- Commented above because it repeats the same function as the website navigation bar -->

<!-- Should eventually separate in tutorials/panelists/etc -->

<div class="tab-wrapper">
	<div class="tab">
	<button class="tablinks" onclick="openSection(event, 'Keynotes')" id="defaultOpen"><strong>Keynotes</strong></button>
	<button class="tablinks" onclick="openSection(event, 'Tutorials')"><strong>Tutorials</strong></button>
	<button class="tablinks" onclick="openSection(event, 'Panels')"><strong>Panels</strong></button>
	</div>
</div>

<div id="Tutorials" class="tabcontent">
 <section id="main-bio">
			<!-- h2: Tier-2 Headline (Not as important as the h1 header, but more than one allowed). -->
			<h3>Dr. Sudipta Kar</h3>
      <h4>Senior Applied Scientist at Amazon AGI</h4>
      <br />
			<!-- Headshot image -->
			<img width="225" id="bio-image" src="/assets/images/speakers/speaker_sudipta.jpeg" align="left" alt="speaker_sudipta" hspace="10" />
     Sudipta Kar is a Senior Applied Scientist at Amazon AGI. He received his Ph.D. in Computer Science from the University of Houston in 2020 under the supervision of Thamar Solorio. His doctoral research focused on creative text analysis. Currently, he works on developing intelligent systems to enable seamless proactivity in smart voice assistants such as Alexa. His research interests include computational systems for low-resource languages, language models, and information extraction.  He has co-organized multiple natural language processing workshops and shared tasks, including BLP, CALCS, MultiCoNER, and SentiMix. Additionally, in 2023 he led the first NLP hackathon held in Bangladesh. 
		</section>
		<section id="additional-info">
      <h4>Tutorial title: The Power of Rewards - Reinforcing Language Models with Reinforcement Learning </h4>
	  Language models ranging from BERT to GPT have shown impressive performance on many natural language tasks through first self-supervised pre-training on large text corpora and then fine-tuning on downstream tasks or even with zero-shot or few-shot approaches. These models are also very powerful in solving multiple tasks. However, their capabilities are still limited as they lack a built-in reward signal to directly optimize for the end task objective or align to certain human preferences.On the other hand, Reinforcement Learning (RL) is another area in machine learning which is commonly applied to develop systems that improve over real-time feedback loops (such as games). Because it provides a framework for optimizing goal-directed behavior through rewards. In this tutorial, we will explore how reinforcement learning came into the play of language modeling and suddenly changed the game by reinforcing the representational power of large language models with the ability to more efficiently solve tasks requiring reasoning, planning, and knowledge. We will first provide background on contemporary language models and reinforcement learning fundamentals. Next, we will discuss techniques for applying policy gradient methods to fine-tune language models to maximize rewards from an environment. Attendees will learn how to practically apply RL to language tasks, understand tradeoffs between different algorithms, and gain insight into state-of-the-art research in this emerging field. 
		</section>
		<hr />
		<section id="main-bio">
			<!-- h2: Tier-2 Headline (Not as important as the h1 header, but more than one allowed). -->
			<h3>Dr. Danae Sánchez</h3>
      <h4>Researcher at the University of Copenhagen</h4>
      <br />
			<!-- Headshot image -->
			<img width="225" id="bio-image" src="/assets/images/speakers/speaker_danae.jpeg" align="left" alt="speaker_danae" hspace="10" />
     Danae Sánchez Villegas is a postdoctoral researcher at the University of Copenhagen. She holds a Ph.D. and a Master's degree in Computer Science from the University of Sheffield and a Bachelor's degree in Computer Engineering from the Instituto Tecnológico Autónomo de México. Her research interests include multilingual natural language understanding, vision and language modeling, and computational social science. Danae has worked as a Research Associate in the Natural Language Processing Group at the University of Sheffield and as an Applied Scientist Intern at Amazon Alexa.
		</section>
		<section id="additional-info">
      <h4>Tutorial title: Exploring Transformers and Limitations in Language Modeling. </h4>
      This tutorial explores language modeling techniques in Natural Language Processing (NLP), covering key concepts from traditional approaches to Transformer architectures. Beginning with an introduction to NLP and language modeling, it delves into probabilistic language models and progresses to neural language models, emphasizing the significance of embeddings for semantic representation. Moving on to Transformer models, we will discuss key concepts such as multi-head attention mechanisms, masked language modeling, and encoder models. Additionally, the tutorial addresses the limitations of large language models, providing insights into challenges and considerations for leveraging these models effectively in practical applications.
		</section>
		<hr />
		<section id="main-bio">
			<!-- h2: Tier-2 Headline (Not as important as the h1 header, but more than one allowed). -->
			<h3>Dr. Alham Fikri Aji</h3>
      <h4>Assistant Professor at MBZUAI, UAE.</h4>
      <br />
			<!-- Headshot image -->
			<img width="225" id="bio-image" src="/assets/images/speakers/speaker_aji.jpeg" align="left" alt="speaker_aji" hspace="10" />
     Dr. Alham Fikri Aji is an Assistant Professor at MBZUAI, holding a Ph.D. from the University of Edinburgh's Institute for Language, Cognition, and Computation. His doctoral research, supervised by Dr. Kenneth Heafield and Dr. Rico Sennrich, focused on enhancing the training and inference speed of machine translation. Dr. Aji's current research centers around multilingual, low-resource, and low-compute Natural Language Processing (NLP). His recent work has been in developing diverse multilingual large language models. and multilingual NLP resources, particularly for underrepresented languages, with a specific emphasis on Indonesian. Dr. Aji has worked at Amazon, Google, and Apple in the past. 
		</section>
		<section id="additional-info">
      <h4>Tutorial title: Training Lightweight Model via Knowledge Distillation and Parameter Efficient Finetuning </h4>
		</section>
	<hr />
	<section id="main-bio">
			<!-- h2: Tier-2 Headline (Not as important as the h1 header, but more than one allowed). -->
			<h3>Dr. Víctor Mijangos</h3>
      <h4>Professor at UNAM, Mexico.</h4>
      <br />
			<img width="225" id="bio-image" src="/assets/images/speakers/speaker_victor.jpeg" align="left" alt="speaker_victor" hspace="10" /> 
		Víctor Mijangos de la Cruz is a Full-Time Professor at the Faculty of Sciences, where he teaches subjects related to Artificial Intelligence, Neural Networks, and Computational Linguistics. In addition to teaching, he develops research projects aimed at exploring inductive biases in neural networks and the application of deep learning models for the development of technologies in indigenous languages. His interests lie in the study of the capabilities and limitations of deep learning, computational linguistics, and the creation of technologies for indigenous languages, particularly in the Otomí language.
		</section>
		<section id="additional-info">
      <h4>Tutorial title: Introduction to Attention Mechanisms in Transformers.</h4>
	  Attention layers are currently central mechanisms in language models. Transformers, which represent the state of the art in this field, rely on the use of attention layers in combination with other strategies. Attention has also been used in models based on sequence-to-sequence recurrent networks, providing significant improvements in natural language processing tasks such as machine translation and text generation. Understanding how these mechanisms work is essential to comprehend current language models. This workshop aims to present a first approach to the attention mechanisms used in neural networks. Firstly, the basic theoretical concepts to understand attention and its operation will be presented, other attention mechanisms, mainly sparse attention, will be reviewed, and the relationship of attention with auto-encoded and auto-regressive language models will be discussed. Finally, its relationship with other mechanisms such as convolutional layers and graph layers, highlighting their advantages and disadvantages, will be addressed.
Secondly, the technical principles for the implementation of attention mechanisms in Pytorch and their incorporation within the architecture of Transformers will be covered.
		</section>
</div>

<div id="Keynotes" class="tabcontent">
<section id="main-bio">
			<!-- h2: Tier-2 Headline (Not as important as the h1 header, but more than one allowed). -->
			<h3>Dr. Diyi Yang</h3>
      <h4> Assistant Professor at Stanford</h4>
      <br />
			<!-- Headshot image -->
			<img width="225" id="bio-image" src="/assets/images/speakers/speaker_diyi.jpeg" align="left" alt="Diyi Yang" hspace="10" />
     	Diyi Yang is an assistant professor in the Computer Science Department at Stanford, affiliated with the Stanford NLP Group, Stanford HCI Group, Stanford AI Lab (SAIL), and Stanford Human-Centered Artificial Intelligence (HAI). Her research interest lies in Socially Aware Natural Language Processing, aiming to better understand human communication in social contexts and develop socially aware language technologies that enhance both human-human and human-computer interaction.
		</section>
	<hr />
  <section id="main-bio">
			<!-- h2: Tier-2 Headline (Not as important as the h1 header, but more than one allowed). -->
			<h3>Dr. Veronica Perez Rosas</h3>
      <h4>Researcher at the University of Michigan</h4>
      <br />
			<!-- Headshot image -->
			<img width="225" id="bio-image" src="/assets/images/speakers/speaker_veronica_perez.jpeg" align="left" alt="Veronica Perez Rosas" hspace="10" />
     She obtained her Ph.D. in Computer Science and Engineering from the University of North Texas in 2014. She is a Level I Researcher recognized by the National System of Researchers. Currently, she is a researcher at the University of Michigan, where she is a part of the Artificial Intelligence laboratory and the Inform Language and Information Technologies research group in the Department of Computer Science. Her research interests include natural language processing (NLP), machine learning, computational linguistics, and multimodal representations. Her research focuses on NLP applications, including automatic detection of misinformation, NLP in mental health, as well as the detection of human behaviors such as sentiment, deception, sarcasm, and affective response. More information about her research can be found at https://vrncapr.engin.umich.edu/.
	 </section>
	 <hr />
	<section id="main-bio">
			<!-- h2: Tier-2 Headline (Not as important as the h1 header, but more than one allowed). -->
			<h3>Dr. Alexis Palmer</h3>
      <h4>Assistant Professor at the University of Colorado Boulder</h4>
      <br />
			<!-- Headshot image -->
			<img width="140" id="bio-image" src="/assets/images/speakers/speaker_alexis.png" align="left" alt="Alexis Palmer" hspace="10" />
     	Alexis Palmer is a computational linguist and a professor in the Department of Linguistics at the University of Colorado Boulder. She studied English literature at the University of Michigan, and later earned both an MA and a PhD in computational linguistics from the University of Texas at Austin. Her main research interests include computational semantics, computational discourse, and the development and application of computational methods to support language documentation and revitalization.
		</section>
	<hr />
</div>

<div id="Panels" class="tabcontent">
  		<section id="main-bio">
			<!-- h2: Tier-2 Headline (Not as important as the h1 header, but more than one allowed). -->
			<h3>Dr. Jocelyn Dunstan</h3>
      <h4>Assistant Professor at the Pontifical Catholic University of Chile.</h4>
      <br />
			<!-- Headshot image -->
			<img width="225" id="bio-image" src="/assets/images/speakers/speaker_jocelyn.jpeg" align="left" alt="speaker_jocelyn" hspace="10" />
     Jocelyn Dunstan is an Assistant Professor at the Pontifical Catholic University of Chile. She holds a Ph.D. in Applied Mathematics and Theoretical Physics from the University of Cambridge in the UK. She specializes in leveraging machine learning and natural language processing to address key challenges. Her research primarily revolves around clinical text mining and patient prioritization. In addition to her academic role at the Catholic University of Chile, she is actively engaged as a researcher at prominent institutions such as the Millenium Institute for Foundational Research on Data (IMFD) and the Advanced Center for Electrical and Electronic Engineering (AC3E). Further information about her group's work can be found on their webpage at pln.cmm.uchile.cl. 
		</section>
	<hr />
	<section id="main-bio">
			<!-- h2: Tier-2 Headline (Not as important as the h1 header, but more than one allowed). -->
			<h3>Dr. Manuel Montes y Gomez</h3>
      <h4>Full Professor at INAOE, Mexico.</h4>
      <br />
			<!-- Headshot image -->
			<img width="225" id="bio-image" src="/assets/images/speakers/speaker_manuel.png" align="left" alt="speaker_jocelyn" hspace="10" />
     Manuel Montes-y-Gómez is Full Professor at the National Institute of Astrophysics, Optics and Electronics (INAOE) of Mexico. His research is on automatic text processing. He is author of more than 250 journal and conference papers in the fields of information retrieval, text mining and authorship analysis. <br /> He has been visiting professor at the Polytechnic University of Valencia (Spain), and the University of Alabama (USA). He is also a member of the Mexican Academy of Sciences (AMC), and founding member of the Mexican Academy of Computer Science (AMEXCOMP), the Mexican Association of Natural Language Processing (AMNLP), and of the Language Technology Network of CONACYT. In the context of them, he has been the organizer of the National Workshop on Language Technologies (from 2004 to 2016), the Mexican Workshop on Plagiarism Detection and Authorship Analysis (2016-2020), the Mexican Autumn School on Language Technologies (2015 and 2016), and a shared task on author profiling, aggressiveness analysis and fake news detection in Mexican Spanish at IberLEF (2018-2021).
		</section>
	<hr />
	<section id="main-bio">
			<!-- h2: Tier-2 Headline (Not as important as the h1 header, but more than one allowed). -->
			<h3>Dr. Luciana Benotti</h3>
      <h4>Associate Professor at the National University of Córdoba and AI Researcher at CONICET, Argentina.</h4>
      <br />
			<!-- Headshot image -->
			<img width="225" id="bio-image" src="/assets/images/speakers/speaker_benotti.jpg" align="left" alt="speaker_benotti" hspace="10" />
     Luciana Benotti is an Associate Professor in Computer Science at the National University of Córdoba and a Researcher in Artificial Intelligence at CONICET, Argentina. Her research interests include different aspects of situated and interactive NLP, such as interpreting instructions in a dialogue, generating contextualized questions, and deciding when to speak in a dialogue system, among others. She is particularly interested in how linguistic and non-linguistic features contribute to the meaning conveyed during a conversation. These features include what the conversational participants are doing while they talk, the visual context, temporal aspects, etc. She has been a visiting scientist at the University of Trento (2019), Stanford University (2018), Roskilde University (2014), the University of Costa Rica (2012), and the University of Southern California (2010). She holds a joint MSc Erasmus Mundus from the Free University of Bolzano and the Polytechnic University of Madrid and a PhD from the Université de Lorraine. This year, she was chosen as the Latin American representative for the North American Association for Computational Linguistics (NAACL).
		</section>
		<hr />
		<section id="main-bio">
			<h3>Dr. Veronica Perez Rosas</h3>
      <h4>Researcher at the University of Michigan</h4>
      <br />
			<!-- Headshot image -->
			<img width="225" id="bio-image" src="/assets/images/speakers/speaker_veronica_perez.jpeg" align="left" alt="Veronica Perez Rosas" hspace="10" />
     She obtained her Ph.D. in Computer Science and Engineering from the University of North Texas in 2014. She is a Level I Researcher recognized by the National System of Researchers. Currently, she is a researcher at the University of Michigan, where she is a part of the Artificial Intelligence laboratory and the Inform Language and Information Technologies research group in the Department of Computer Science. Her research interests include natural language processing (NLP), machine learning, computational linguistics, and multimodal representations. Her research focuses on NLP applications, including automatic detection of misinformation, NLP in mental health, as well as the detection of human behaviors such as sentiment, deception, sarcasm, and affective response. More information about her research can be found at https://vrncapr.engin.umich.edu/.
	</section>
	<hr />
</div>

<script>
// JavaScript for Tabs
function openSection(evt, sectionName) {
  var i, tabcontent, tablinks;
  tabcontent = document.getElementsByClassName("tabcontent");
  for (i = 0; i < tabcontent.length; i++) {
    tabcontent[i].style.display = "none";
  }
  tablinks = document.getElementsByClassName("tablinks");
  for (i = 0; i < tablinks.length; i++) {
    tablinks[i].className = tablinks[i].className.replace(" active", "");
  }
  document.getElementById(sectionName).style.display = "block";
  evt.currentTarget.className += " active";
}

// Open the first tab by default
document.addEventListener("DOMContentLoaded", function() {
  document.getElementById("defaultOpen").click();
});
</script>

<!-- <button class="tablinks" onclick="openSection(event, 'Tutorials')" id="defaultOpen">Bio</button> -->



      

      

    </div>
  </div>
</div>


  <footer>
  <div class="container-md beautiful-jekyll-footer">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
      <ul class="list-inline text-center footer-links"><li class="list-inline-item">
    <a href="mailto:esau.villatoro@idiap.ch" title="Email me">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Email me</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://github.com/ampln" title="GitHub">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">GitHub</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://twitter.com/AMPLN" title="Twitter">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Twitter</span>
   </a>
  </li></ul>

      
      <p class="copyright text-muted">
      
        AMPLN
        &nbsp;&bull;&nbsp;
      
      2024

      

      
      </p>
      <p class="theme-by text-muted">
        Powered by
        <a href="https://beautifuljekyll.com">Beautiful Jekyll</a>
      </p>
      </div>
    </div>
  </div>
</footer>


  
  
    
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=" crossorigin="anonymous"></script>


  
    
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


  
    
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


  



  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script src="/assets/js/beautifuljekyll.js"></script>
    
  









</body>
</html>
