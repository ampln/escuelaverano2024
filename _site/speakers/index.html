<!DOCTYPE html>
<html lang="en">
<!-- Beautiful Jekyll 5.0.0 | Copyright Dean Attali 2020 -->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

  

  <title>Speakers</title>

  
  <meta name="author" content="AMPLN">
  

  <meta name="description" content="Keynotes Tutorials Panels Sudipta Kar Senior Applied Scientist at Amazon AGI Sudipta Kar is a Senior Applied Scientist at Amazon AGI. He received his Ph.D. in Computer Science from the University of Houston in 2020 under the supervision of Thamar Solorio. His doctoral research focused on creative text analysis. Currently,...">

  

  

  <link rel="alternate" type="application/rss+xml" title="Mexican NLP Summer School 2024" href="http://localhost:4000/feed.xml">

  

  

  


  
    
      
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">


    
      
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800">


    
  

  
    
      <link rel="stylesheet" href="/assets/css/bootstrap-social.css">
    
      <link rel="stylesheet" href="/assets/css/beautifuljekyll.css">
    
  

  

  
  
  
    
      <link rel="stylesheet" href="/assets/css/index.css">
    
  

  

  
  <meta property="og:site_name" content="Mexican NLP Summer School 2024">
  <meta property="og:title" content="Speakers">
  <meta property="og:description" content="Keynotes Tutorials Panels Sudipta Kar Senior Applied Scientist at Amazon AGI Sudipta Kar is a Senior Applied Scientist at Amazon AGI. He received his Ph.D. in Computer Science from the University of Houston in 2020 under the supervision of Thamar Solorio. His doctoral research focused on creative text analysis. Currently,...">

  

  
  <meta property="og:type" content="website">
  <meta property="og:url" content="http://localhost:4000/speakers/">
  <link rel="canonical" href="http://localhost:4000/speakers/">
  

  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:site" content="@AMPLN">
  <meta name="twitter:creator" content="@AMPLN">

  <meta property="twitter:title" content="Speakers">
  <meta property="twitter:description" content="Keynotes Tutorials Panels Sudipta Kar Senior Applied Scientist at Amazon AGI Sudipta Kar is a Senior Applied Scientist at Amazon AGI. He received his Ph.D. in Computer Science from the University of Houston in 2020 under the supervision of Thamar Solorio. His doctoral research focused on creative text analysis. Currently,...">

  

  


  

  

</head>


<body>

  


  <nav class="navbar navbar-expand-xl navbar-light fixed-top navbar-custom top-nav-regular"><a class="navbar-brand navbar-brand-logo" href="http://localhost:4000/"><img alt="Mexican NLP Summer School 2024 Logo" src="/assets/images/ampln_header.png"/></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="main-navbar">
    <ul class="navbar-nav ml-auto">
          <li class="nav-item dropdown">
            <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Program information</a>
            <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="/speakers">Speakers</a>
                  <a class="dropdown-item" href="/program">Program</a>
            </div>
          </li>
        
          <li class="nav-item">
            <a class="nav-link" href="/registration">Registration</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/fqa">FQA</a>
          </li>
          <li class="nav-item dropdown">
            <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">About the school</a>
            <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="/about">About</a>
                  <a class="dropdown-item" href="/chairs">Organizers</a>
                  <a class="dropdown-item" href="/previousEditions">Previous editions</a>
                  <a class="dropdown-item" href="/CodigoConducta">Code of Conduct</a>
            </div>
          </li>
        
          <li class="nav-item">
            <a class="nav-link" href="/sponsors">Sponsors</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/contact">Contact</a>
          </li></ul>
  </div>

  

  

</nav>


  <!-- TODO this file has become a mess, refactor it -->







<header class="header-section ">

<div class="intro-header no-img">
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="page-heading">
          <h1>Speakers</h1>
          

          
        </div>
      </div>
    </div>
  </div>
</div>
</header>





<div class=" container-md " role="main">
  <div class="row">
    <div class=" col-xl-8 offset-xl-2 col-lg-10 offset-lg-1 ">
      

      <!-- <div class="list-filters">
  <a href="/escuelaverano2024/" class="list-filter">Home</a>
  <a href="/escuelaverano2024/speakers/" class="list-filter filter-selected">Speakers</a>
  <a href="/escuelaverano2024/program/" class="list-filter">Program</a>
  <a href="/escuelaverano2024/about/" class="list-filter">About</a>
</div> -->
<!-- Commented above because it repeats the same function as the website navigation bar -->

<!-- Should eventually separate in tutorials/panelists/etc -->

<div class="tab-wrapper">
	<div class="tab">
	<button class="tablinks" onclick="openSection(event, 'Keynotes')" id="defaultOpen"><strong>Keynotes</strong></button>
	<button class="tablinks" onclick="openSection(event, 'Tutorials')"><strong>Tutorials</strong></button>
	<button class="tablinks" onclick="openSection(event, 'Panels')"><strong>Panels</strong></button>
	</div>
</div>

<div id="Tutorials" class="tabcontent">
 <section id="main-bio">
			<!-- h2: Tier-2 Headline (Not as important as the h1 header, but more than one allowed). -->
			<h3>Sudipta Kar</h3>
      <h4>Senior Applied Scientist at Amazon AGI</h4>
      <br />
			<!-- Headshot image -->
			<img width="225" id="bio-image" src="/assets/images/speakers/speaker_sudipta.jpeg" align="left" alt="speaker_sudipta" hspace="10" />
     <a href="https://sudiptakar.info/">Sudipta Kar</a> is a Senior Applied Scientist at Amazon AGI. He received his Ph.D. in Computer Science from the University of Houston in 2020 under the supervision of Thamar Solorio. His doctoral research focused on creative text analysis. Currently, he works on developing intelligent systems to enable seamless proactivity in smart voice assistants such as Alexa. His research interests include computational systems for low-resource languages, language models, and information extraction.  He has co-organized multiple natural language processing workshops and shared tasks, including BLP, CALCS, MultiCoNER, and SentiMix. Additionally, in 2023 he led the first NLP hackathon held in Bangladesh. 
		</section>
		<section id="additional-info">
      <h4>Tutorial title: "<i>The Power of Rewards - Reinforcing Language Models with Reinforcement Learning</i>" </h4>
	  Language models ranging from BERT to GPT have shown impressive performance on many natural language tasks through first self-supervised pre-training on large text corpora and then fine-tuning on downstream tasks or even with zero-shot or few-shot approaches. These models are also very powerful in solving multiple tasks. However, their capabilities are still limited as they lack a built-in reward signal to directly optimize for the end task objective or align to certain human preferences.On the other hand, Reinforcement Learning (RL) is another area in machine learning which is commonly applied to develop systems that improve over real-time feedback loops (such as games). Because it provides a framework for optimizing goal-directed behavior through rewards. In this tutorial, we will explore how reinforcement learning came into the play of language modeling and suddenly changed the game by reinforcing the representational power of large language models with the ability to more efficiently solve tasks requiring reasoning, planning, and knowledge. We will first provide background on contemporary language models and reinforcement learning fundamentals. Next, we will discuss techniques for applying policy gradient methods to fine-tune language models to maximize rewards from an environment. Attendees will learn how to practically apply RL to language tasks, understand tradeoffs between different algorithms, and gain insight into state-of-the-art research in this emerging field. 
		</section>
		<hr />
		<section id="main-bio">
			<!-- h2: Tier-2 Headline (Not as important as the h1 header, but more than one allowed). -->
			<h3>Danae Sánchez</h3>
      <h4>Researcher at the University of Copenhagen</h4>
      <br />
			<!-- Headshot image -->
			<img width="225" id="bio-image" src="/assets/images/speakers/speaker_danae.jpeg" align="left" alt="speaker_danae" hspace="10" />
     <a href="https://danaesavi.github.io/">Danae Sánchez Villegas</a> is a postdoctoral researcher at the University of Copenhagen. She holds a Ph.D. and a Master's degree in Computer Science from the University of Sheffield and a Bachelor's degree in Computer Engineering from the Instituto Tecnológico Autónomo de México. Her research interests include multilingual natural language understanding, vision and language modeling, and computational social science. Danae has worked as a Research Associate in the Natural Language Processing Group at the University of Sheffield and as an Applied Scientist Intern at Amazon Alexa.
		</section>
		<section id="additional-info">
      <h4>Tutorial title: "<i>Exploring Transformers and Limitations in Language Modeling</i>" </h4>
      This tutorial explores language modeling techniques in Natural Language Processing (NLP), covering key concepts from traditional approaches to Transformer architectures. Beginning with an introduction to NLP and language modeling, it delves into probabilistic language models and progresses to neural language models, emphasizing the significance of embeddings for semantic representation. Moving on to Transformer models, we will discuss key concepts such as multi-head attention mechanisms, masked language modeling, and encoder models. Additionally, the tutorial addresses the limitations of large language models, providing insights into challenges and considerations for leveraging these models effectively in practical applications.
		</section>
		<hr />
		<section id="main-bio">
			<!-- h2: Tier-2 Headline (Not as important as the h1 header, but more than one allowed). -->
			<h3>Alham Fikri Aji</h3>
      <h4>Assistant Professor at MBZUAI, UAE.</h4>
      <br />
			<!-- Headshot image -->
			<img width="225" id="bio-image" src="/assets/images/speakers/speaker_aji.jpeg" align="left" alt="speaker_aji" hspace="10" />
     <a href="https://afaji.github.io/">Alham Fikri Aji</a> is an Assistant Professor at MBZUAI, holding a Ph.D. from the University of Edinburgh's Institute for Language, Cognition, and Computation. His doctoral research, supervised by Dr. Kenneth Heafield and Dr. Rico Sennrich, focused on enhancing the training and inference speed of machine translation. Dr. Aji's current research centers around multilingual, low-resource, and low-compute Natural Language Processing (NLP). His recent work has been in developing diverse multilingual large language models. and multilingual NLP resources, particularly for underrepresented languages, with a specific emphasis on Indonesian. He has worked at Amazon, Google, and Apple in the past. 
		</section>
		<section id="additional-info">
      <h4>Tutorial title: "<i>Training Lightweight Model via Knowledge Distillation and Parameter Efficient Finetuning</i>" </h4>
	  Language models can be resource-hungry and prohibitive to train and deploy for many people due to a lack of proper computing resources. In this tutorial, we delve into how to build smaller, more lightweight models without sacrificing quality via knowledge distillation. Additionally, we delve into training your model with less memory usage through parameter-efficient finetuning. In this tutorial, we will delve into the hands-on implementation. It is ideal for those just delving into the field of AI/NLP.
		</section>
	<hr />
	<section id="main-bio">
			<!-- h2: Tier-2 Headline (Not as important as the h1 header, but more than one allowed). -->
			<h3>Víctor Mijangos</h3>
      <h4>Professor at UNAM, Mexico.</h4>
      <br />
			<img width="225" id="bio-image" src="/assets/images/speakers/speaker_victor.jpeg" align="left" alt="speaker_victor" hspace="10" /> 
		<a href="https://sites.google.com/site/victormijangoscruz/">Víctor Mijangos de la Cruz</a> is a Full-Time Professor at the Faculty of Sciences, where he teaches subjects related to Artificial Intelligence, Neural Networks, and Computational Linguistics. In addition to teaching, he develops research projects aimed at exploring inductive biases in neural networks and the application of deep learning models for the development of technologies in indigenous languages. His interests lie in the study of the capabilities and limitations of deep learning, computational linguistics, and the creation of technologies for indigenous languages, particularly in the Otomí language.
		</section>
		<section id="additional-info">
      <h4>Tutorial title: "<i>Introduction to Attention Mechanisms in Transformers</i>"</h4>
	  Attention layers are currently central mechanisms in language models. Transformers, which represent the state of the art in this field, rely on the use of attention layers in combination with other strategies. Attention has also been used in models based on sequence-to-sequence recurrent networks, providing significant improvements in natural language processing tasks such as machine translation and text generation. Understanding how these mechanisms work is essential to comprehend current language models. This workshop aims to present a first approach to the attention mechanisms used in neural networks. Firstly, the basic theoretical concepts to understand attention and its operation will be presented, other attention mechanisms, mainly sparse attention, will be reviewed, and the relationship of attention with auto-encoded and auto-regressive language models will be discussed. Finally, its relationship with other mechanisms such as convolutional layers and graph layers, highlighting their advantages and disadvantages, will be addressed.
Secondly, the technical principles for the implementation of attention mechanisms in Pytorch and their incorporation within the architecture of Transformers will be covered.
		</section>
</div>

<div id="Keynotes" class="tabcontent">
<section id="main-bio">
			<!-- h2: Tier-2 Headline (Not as important as the h1 header, but more than one allowed). -->
			<h3>Diyi Yang</h3>
      <h4> Assistant Professor at Stanford</h4>
      <br />
			<!-- Headshot image -->
			<img width="225" id="bio-image" src="/assets/images/speakers/speaker_diyi.jpeg" align="left" alt="headshot" hspace="10" />
     	<a href="https://cs.stanford.edu/~diyiy/">Diyi Yang</a> is an assistant professor in the Computer Science Department at Stanford University, also affiliated with the Stanford NLP Group, Stanford HCI Group and Stanford Human Centered AI Institute. Her research focuses on human-centered natural language processing and computational social science.  She is a recipient of  IEEE “AI 10 to Watch” (2020), Microsoft Research Faculty Fellowship (2021),  NSF CAREER Award (2022), an ONR Young Investigator Award (2023), and a Sloan Research Fellowship (2024).   Her work has received multiple paper awards or nominations at top NLP and HCI conferences, (e.g., Best Paper Honorable Mention at ICWSM 2016, Best Paper Honorable Mention at SIGCHI 2019, and Outstanding Paper at ACL 2022). 
		</section>
		<section id="additional-info">
	<h4>Keynote: "<i>Human Centered NLP for Positive Impact</i>"</h4>
	  Large language models have revolutionized the way humans interact with AI systems, transforming a wide range of fields and disciplines. However, there is a growing amount of evidence and concern about the negative aspects of NLP systems such as biases and the lack of input from users. How can we build NLP systems that are more aware of human factors?  In this talk, we will present two case studies on how human-centered design can be leveraged to build responsible NLP applications. The first one presents a participatory design approach to develop dialect-inclusive language tools and adaptation techniques for low-resourced language and dialect. The second part looks at social skill training with LLMs by demonstrating how we use LLMs to teach conflict resolution skills through simulated practice. We conclude by discussing how human-AI interaction via LLMs can empower individuals and foster positive change.
		</section>
	<hr />
  <section id="main-bio">
			<!-- h2: Tier-2 Headline (Not as important as the h1 header, but more than one allowed). -->
			<h3>Veronica Perez Rosas</h3>
      <h4>Researcher at the University of Michigan</h4>
      <br />
			<!-- Headshot image -->
			<img width="225" id="bio-image" src="/assets/images/speakers/speaker_veronica_perez.jpeg" align="left" alt="Veronica Perez Rosas" hspace="10" />
     <a href="https://vrncapr.engin.umich.edu/">Veronica Perez Rosas</a> is an Assistant Research Scientist at the University of Michigan. Her research interests include Natural Language Processing, Machine Learning,  Affect Recognition, and Multimodal Processing of Human Behavior. Her research focuses on developing computational methods to analyze, recognize, and predict human behaviors during social interactions. She has authored papers in leading conferences and journals in Natural Language Processing and Multimodal Processing, has mentored numerous students in these research areas, and has served as workshop chair or area chair for multiple international conferences in the field.
	 </section>
	 <section id="additional-info">
	 <h4>Keynote: "<i>NLP for Timely and Actionable Feedback in Healthcare Conversations</i>"</h4>
	  Effective communication is essential in mental healthcare,
	  as it influences patient responses, treatment decisions, and outcomes.
	  In this presentation, I'll discuss computational methods aimed at helping clinicians enhance their communication skills during patient interactions, with a particular focus on feedback and coaching in counseling. Firstly, I'll explore the generation of language cues to assist counselors in crafting responses that align with counseling guidelines, using language models augmented with context and common sense. Secondly, I will introduce language-based coaching systems that use contrastive learning and language models for response scoring and rewriting. These tools aim to empower clinicians to refine their communication skills by providing detailed feedback and enabling them to adapt their style to better suit patients' needs. Finally, I'll share a case study demonstrating the application of these tools in educational settings, highlighting the potential of AI in mental healthcare.
		</section>
	 <hr />
	<section id="main-bio">
			<!-- h2: Tier-2 Headline (Not as important as the h1 header, but more than one allowed). -->
			<h3>Alexis Palmer</h3>
      <h4>Assistant Professor at the University of Colorado Boulder</h4>
      <br />
			<!-- Headshot image -->
			<img width="140" id="bio-image" src="/assets/images/speakers/speaker_alexis.png" align="left" alt="Alexis Palmer" hspace="10" />
     	<a href="https://lecs-lab.github.io/">Alexis Palmer</a> is an assistant professor in Linguistics at CU Boulder, a fellow of the ICS, and affiliated faculty with the department of Computer Science. Her primary research focus is on computational linguistics for low-resource and endangered languages. She also works in computational semantics, computational discourse, and computational analysis of toxic language. Her current work includes incorporating linguistic insights into cross-lingual transfer learning in order to more rapidly build tools to support low-resource and endangered languages. She is a member at large of the ACL Special Interest Group on Typology and Multilingual NLP, and a founding member of the ACL Special Interest Group on Endangered Languages. Alexis grew up in Northern Michigan, did her PhD at the University of Texas in Austin, and made her way to Boulder by way of several years in German universities and research institutions, as well as the University of North Texas.
		</section>
		<section id="additional-info">
		<!-- <h4>Keynote: </h4> -->
      <h4>Keynote: "<i>A tricky intersection: Natural language processing and endangered language documentation</i>"</h4>
	  Of the roughly 7000 languages spoken across the globe, more than half have been categorized as endangered, threatened, or of decreasing vitality. Only a handful of the world’s languages have anywhere near the amount of digitally-available data needed to train large language models such as ChatGPT, leading to a dramatic disparity in access to cutting-edge technologies. At the same time, for several decades now, linguists, language activists, language community members, and other researchers have recognized language endangerment as a pressing issue and have devoted considerable time to the incredibly time-consuming work of language documentation and revitalization. And yet, as a field, we have not yet realized the potential of natural language processing (NLP) technologies to support language documentation, description, and revitalization. In this talk, I will discuss some of the structural barriers inherent to the intersection of NLP and endangered language documentation, as well as some important ethical considerations that arise at this intersection. I will also present our work on developing models to partially automate the complex linguistic analysis work involved with producing interlinear glossed text, an important format in language documentation.
		</section>
	<hr />
	<section id="main-bio">
			<!-- h2: Tier-2 Headline (Not as important as the h1 header, but more than one allowed). -->
			<h3>Pablo Rivas</h3>
      <h4>Assistant Professor at Baylor University</h4>
      <br />
			<!-- Headshot image -->
			<img width="140" id="bio-image" src="/assets/images/speakers/pablo_rivas_pic.jpeg" align="left" alt="Pablo Rivas" hspace="10" />
     	<a href="https://www.rivas.ai/">Pablo Rivas</a> is an Assistant Professor of Computer Science at Baylor University, where he directs the NSF IUCRC Center for Standards and Ethics in Artificial Intelligence (planning). He holds a Ph.D. in Electrical and Computer Engineering from the University of Texas at El Paso and completed his masters and undergraduate studies at the Tecnologico Nacional de Mexico. His research primarily focuses on natural language processing (NLP), responsible AI, and machine learning. Rivas' recent contributions to the field of NLP include aligning word embeddings from BERT to vocabulary-free representations and developing automated assessment systems for SQL statements. His work also extends to combating human trafficking through NLP by analyzing online advertisements. A Senior Member of IEEE, ACM, and SIAM, Rivas is committed to advancing responsible AI practices and has been recognized with numerous awards and honors for his contributions.
		</section>
		<section id="additional-info">
		<h4>Keynote: "<i>Combatting Human Trafficking in the Cyberspace: An NLP-Based Methodology to Analyze Online Advertisements</i>"</h4>
	  Human trafficking is a grave issue proliferating in online marketplaces. This keynote will discuss an advanced NLP methodology designed to combat this problem by analyzing the language used in online advertisements. The approach leverages pseudo-labeled datasets and minimal supervision to train state-of-the-art NLP models, focusing on Human Trafficking Risk Prediction (HTRP) and Organized Activity Detection (OAD). Cutting-edge transformer models and an interpretability framework based on integrated gradients are utilized to provide explainable insights crucial for law enforcement. This work, funded by an NSF multidisciplinary award, addresses a critical gap in the literature and offers a scalable, machine learning-driven solution to combat online human trafficking. The keynote will explore the intersection of NLP and social good, demonstrating how technology can be pivotal in addressing complex social challenges.
		</section>
	<hr />
</div>

<div id="Panels" class="tabcontent">
    <style>
        /* Basic Reset */
        details { padding: 10px; margin: 10px; background: #f0f0f0; border-radius: 8px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }
        summary { cursor: pointer; }
        summary::-webkit-details-marker { display: none; } /* Hide default arrow icon in Chrome */
        /* Panel Styling */
        .panel-summary {
            font-size: 18px;
            font-weight: bold;
            color: #333;
            padding: 10px;
            border-bottom: 1px solid #ccc;
            background-color: #e9ecef;
            border-radius: 5px;
            outline: none;
        }
        /* Speaker Styling */
        .speaker-summary {
            font-size: 16px;
            font-weight: normal;
            color: #555;
            padding-left: 20px;
            border-bottom: 1px solid #ddd;
            outline: none;
        }
    </style>
    <details>
        <summary class="panel-summary">From "publish or perish" to "publicize or perish": Navigating challenges and opportunities for career advancement in Latin American communities in the current era of scientific publication trends.</summary>
        <!-- <p>The ACL recently changed its <a href="https://www.aclweb.org/adminwiki/index.php/ACL_Anonymity_Policy">anonymity policy</a> and now allows authors to post deanonymized preprints anytime, including during the peer review process of papers. To read more about the working group that made this recommendation, go <a href="https://www.aclweb.org/adminwiki/images/5/56/ACL_Anonymity_Policy.pdf">here</a>.<br>
		Latin America (LATAM) communities are fragmented from the mainstream research community, with publication practices dictated by an evaluation system that mostly rewards journal publications. In fact, the preprint culture in LATAM is so foreign that in some circles arxiving is a practice that is still confusing, if not outright unknown, to many. Yet, if LATAM researchers want to engage with the global north and increase the visibility of their research, then they need to become familiar with the publication trends, such as preprinting, of the global north and adjust publication practices accordingly. In addition to prepritning, our field is also facing pressure to publicize the work in social media platforms, such as X, where studies have shown that papers tweeted by “influencers” in the field accumulate more citations than others. <br>
		This panel aims to bring to light questions from the LATAM community regarding the perceived need to preprint and publicize our work, even before it has been officially accepted for publication in a peer-reviewed venue.
		<hr></p> -->
		<!-- <p><strong>Panelists:</strong></p> -->
        <details>
            <summary class="speaker-summary"><strong>Diyi Yang</strong> | Assistant Professor at Stanford</summary>
			<br />
			<img width="150" id="bio-image" src="/assets/images/speakers/speaker_diyi.jpeg" align="left" alt="headshot" hspace="10" />
            <p><a href="https://cs.stanford.edu/~diyiy/">Diyi Yang</a> is an assistant professor in the Computer Science Department at Stanford University, also affiliated with the Stanford NLP Group, Stanford HCI Group and Stanford Human Centered AI Institute. Her research focuses on human-centered natural language processing and computational social science.  She is a recipient of  IEEE “AI 10 to Watch” (2020), Microsoft Research Faculty Fellowship (2021),  NSF CAREER Award (2022), an ONR Young Investigator Award (2023), and a Sloan Research Fellowship (2024).   Her work has received multiple paper awards or nominations at top NLP and HCI conferences, (e.g., Best Paper Honorable Mention at ICWSM 2016, Best Paper Honorable Mention at SIGCHI 2019, and Outstanding Paper at ACL 2022). </p>
        </details>
		<details>
            <summary class="speaker-summary"><strong>Manuel Montes y Gomez</strong> | Full Professor at the National Institute of Astrophysics, Optics and Electronics (INAOE) of Mexico</summary>
			<br />
			<img width="150" id="bio-image" src="/assets/images/speakers/speaker_manuel.png" align="left" alt="speaker_jocelyn" hspace="10" />
            <p><a href="https://ccc.inaoep.mx/~mmontesg/">Manuel Montes-y-Gómez</a> is Full Professor at the National Institute of Astrophysics, Optics and Electronics (INAOE) of Mexico. His research is on automatic text processing. He is author of more than 250 journal and conference papers in the fields of information retrieval, text mining and authorship analysis. <br /> He has been visiting professor at the Polytechnic University of Valencia (Spain), and the University of Alabama (USA). He is also a member of the Mexican Academy of Sciences (AMC), and founding member of the Mexican Academy of Computer Science (AMEXCOMP), the Mexican Association of Natural Language Processing (AMNLP), and of the Language Technology Network of CONACYT. In the context of them, he has been the organizer of the National Workshop on Language Technologies (from 2004 to 2016), the Mexican Workshop on Plagiarism Detection and Authorship Analysis (2016-2020), the Mexican Autumn School on Language Technologies (2015 and 2016), and a shared task on author profiling, aggressiveness analysis and fake news detection in Mexican Spanish at IberLEF (2018-2021).</p>
        </details>
		<details>
            <summary class="speaker-summary"><strong>Manuel Mager</strong> | Applied Scientist at Amazon AWS</summary>
			<br />
			<img width="150" id="bio-image" src="/assets/images/speakers/speaker_mager.png" align="left" alt="speaker_jocelyn" hspace="10" />
            <p><a href="http://code.kiutz.com/">Manuel Mager</a> is an Applied Scientist at AWS Bedrock. He did his Ph.D. at the University of Stuttgart, Germany, graduated in informatics from the National Autonomous University of Mexico (UNAM) and did a Master's in Computer Science at the Metropolitan Autonomous University, Mexico (UAM). His research is focused on Natural Language Processing for low resource languages, mainly indigenous languages of the American continent that are polysynthetic. He also worked on Graph-to-text generation and information extraction, and LLM alignment. He is currently working on responsible AI..</p>
        </details>
        <hr />
    </details>
    <details>
        <summary class="panel-summary">Perspectivas de NLP desde Latinoamérica / NLP perspectives, an overview from Latin America.</summary>
        <!-- <p>This panel aims to reflect on the panorama of NLP research and applications in Latin America. Participants will share their experience directing research groups and projects in the industry.  They will address the challenges faced in these areas when working in Latin America, as well as the potential areas of growth  and internationalization opportunities 
		<hr>
		Este panel tiene el objetivo de discutir el panorama de la investigación y aplicaciones del PLN en latinoamérica. Las participantes compartirán su experiencia dirigiendo grupos de investigación y proyectos en la industria.  Se abordarán los retos particulares que se enfrentan en estas áreas desde latinoamérica,  áreas de oportunidad, así como una reflexión en torno a oportunidades de internacionalización. 
		<hr>
		Language: Spanish.
		<hr>
		</p> -->
		<!-- <p><strong>Panelists:</strong></p> -->
        <details>
            <summary class="speaker-summary"><strong>Luciana Benotti</strong> | Associate Professor at the National University of Córdoba and AI Researcher at CONICET, Argentina.</summary>
			<br />
			<img width="150" id="bio-image" src="/assets/images/speakers/speaker_benotti.jpg" align="left" alt="speaker_benotti" hspace="10" />
            <p><a href="https://benotti.github.io/">Luciana Benotti</a> is an Associate Professor in Computer Science at the National University of Córdoba and a Researcher in Artificial Intelligence at CONICET, Argentina. Her research interests include different aspects of situated and interactive NLP, such as interpreting instructions in a dialogue, generating contextualized questions, and deciding when to speak in a dialogue system, among others. She is particularly interested in how linguistic and non-linguistic features contribute to the meaning conveyed during a conversation. These features include what the conversational participants are doing while they talk, the visual context, temporal aspects, etc. She has been a visiting scientist at the University of Trento (2019), Stanford University (2018), Roskilde University (2014), the University of Costa Rica (2012), and the University of Southern California (2010). She holds a joint MSc Erasmus Mundus from the Free University of Bolzano and the Polytechnic University of Madrid and a PhD from the Université de Lorraine. This year, she was chosen as the Latin American representative for the North American Association for Computational Linguistics (NAACL).</p>
        </details>
		<details>
            <summary class="speaker-summary"><strong>Helena Gomez-Adorno</strong> | Associate Professor at IIMAS-UNAM, Mexico.</summary>
			<br />
			<img width="150" id="bio-image" src="/assets/images/speakers/speaker_helena.jpeg" align="left" alt="speaker_helena" hspace="10" />
            <p><a href="https://helenagomez-adorno.github.io/">Helena Gómez Adorno</a> is a professor of Artificial Intelligence at IIMAS-UNAM. Among her research interests areas are natural language processing, computational linguistics, and information retrieval; specifically, question answering, semantic similarity, authorship attribution, and author profiling. Lead a research and development team to implement intelligent systems with the help of machine learning and neural networks.</p>
        </details>
		<details>
            <summary class="speaker-summary"><strong>Jocelyn Dunstan</strong> | Assistant Professor at the Pontifical Catholic University of Chile.</summary>
			<br />
			<img width="150" id="bio-image" src="/assets/images/speakers/speaker_jocelyn.jpeg" align="left" alt="speaker_jocelyn" hspace="10" />
            <p><a href="https://sites.google.com/view/jdunstan/home">Jocelyn Dunstan</a> is an Assistant Professor at the Pontifical Catholic University of Chile. She holds a Ph.D. in Applied Mathematics and Theoretical Physics from the University of Cambridge in the UK. She specializes in leveraging machine learning and natural language processing to address key challenges. Her research primarily revolves around clinical text mining and patient prioritization. In addition to her academic role at the Catholic University of Chile, she is actively engaged as a researcher at prominent institutions such as the Millenium Institute for Foundational Research on Data (IMFD) and the Advanced Center for Electrical and Electronic Engineering (AC3E). Further information about her group's work can be found on their webpage at pln.cmm.uchile.cl. </p>
        </details>
		<details>
            <summary class="speaker-summary"><strong>Aidé Paola Ríos Rivero</strong> | Consultant at Deloitte</summary>
			<br />
			<img width="150" id="bio-image" src="/assets/images/speakers/speaker_aide.jpeg" align="left" alt="headshot" hspace="10" />
            <p>Aidé Ríos Rivero studied Linguistics at the National Autonomous University of Mexico. She currently works as a consultant at Deloitte in the Conversational AI area. Aide has worked on several linguistic projects such as transcription, analysis of lexical and argument structure of verbs, annotation of automatic translation errors, research into dialectal variants of Spanish, etc. In recent years, she has worked mainly on the design, deployment, and improvement of voice portals for telephony, banking, and finance companies Her main interests are Natural Language Processing and Conversational Design.</p>
        </details>
        <hr />
    </details>
	<details>
        <summary class="panel-summary">Getting into NLP: Insights from experts and peers.</summary>
        <!-- <p>Recent advances in natural language processing (NLP) have helped the field to rapidly grow in popularity and become extensively used. NLP is an exciting subfield of AI at the intersection of computer science, linguistics and cognitive science. Due to its multidisciplinary nature, venturing into this field can be intimidating to newcomers and those without a background in one of those core disciplines. “How hard is it to get into NLP?”, “Should I pursue a Master’s or a Ph.D.?”, “What does the job market look like?” are some of the most common concerns among those new to the field.<br>
		This panel aims to answer these questions by creating an interactive space to discuss the various aspects involved in working in NLP. Attendees can expect to hear firsthand experiences and practical advice from experienced professionals and graduate students at various stages of their academic journeys. This panel will be particularly valuable for students, recent graduates, or professionals considering a career switch to NLP.
		<hr>
		</p> -->
		<!-- <p><strong>Panelists:</strong></p> -->
        <details>
            <summary class="speaker-summary"><strong>Alexis Palmer</strong> | Assistant Professor at the University of Colorado Boulder</summary>
			<br />
			<img width="150" id="bio-image" src="/assets/images/speakers/speaker_alexis.png" align="left" alt="Alexis Palmer" hspace="10" />
            <p><a href="https://lecs-lab.github.io/">Alexis Palmer</a> is an assistant professor in Linguistics at CU Boulder, a fellow of the ICS, and affiliated faculty with the department of Computer Science. Her primary research focus is on computational linguistics for low-resource and endangered languages. She also works in computational semantics, computational discourse, and computational analysis of toxic language. Her current work includes incorporating linguistic insights into cross-lingual transfer learning in order to more rapidly build tools to support low-resource and endangered languages. She is a member at large of the ACL Special Interest Group on Typology and Multilingual NLP, and a founding member of the ACL Special Interest Group on Endangered Languages. Alexis grew up in Northern Michigan, did her PhD at the University of Texas in Austin, and made her way to Boulder by way of several years in German universities and research institutions, as well as the University of North Texas.</p>
        </details>
		<details>
            <summary class="speaker-summary"><strong>Roma Patel</strong> | Research Scientist at Google DeepMind</summary>
			<br />
			<img width="150" id="bio-image" src="/assets/images/speakers/speaker_roma.jpeg" align="left" alt="headshot" hspace="10" />
            <p><a href="https://roma-patel.github.io/">Roma Patel</a> is a Senior Research Scientist at Deepmind, where her research focuses on grounded language learning i.e., teaching agents to understand and use language for more intelligent behaviour, interpretability (mechanistically understanding how language models work) and safety (building more inclusive, safer and less biased language models). Given the interdisciplinary nature of this research agenda, Roma's research has spanned the fields of robotics, multi-agent reinforcement learning and natural language processing. She finished her PhD at Brown University in 2018 advised by Ellie Pavlick.</p>
        </details>
		<details>
            <summary class="speaker-summary"><strong>Emilio Villa-Cueva</strong> | Master’s student at CIMAT, Mexico.</summary>
			<br />
			<img width="150" id="bio-image" src="/assets/images/speakers/speaker_emilio.png" align="left" alt="headshot" hspace="10" />
            <p><a href="https://villacu.github.io/">Emilio Villa-Cueva</a> holds a degree in Engineering Physics from the University of Guanajuato. Currently, he is a master’s student in computer science at CIMAT, specializing in Natural Language Processing. He has previously worked on domain adaptation, question-answering, few-shot learning, and cross-lingual transfer. His current interests are multilingual LLMs and their applications.</p>
        </details>
		<details>
            <summary class="speaker-summary"><strong>Selene Báez Santamaría</strong> | PhD candidate at Vrije Universiteit Amsterdam</summary>
			<br />
			<img width="150" id="bio-image" src="/assets/images/speakers/speaker_selene.jpeg" align="left" alt="headshot" hspace="10" />
            <p><a href="https://selbaez.github.io/">Selene Báez Santamaría</a> is a PhD candiate at Vrije Universiteit Amsterdam. Her research interests are natural language processing, and knowledge representation, focusing on the design of conversational social agents.</p>
        </details>
		<details>
            <summary class="speaker-summary"><strong>Horacio Jarquin</strong> | PhD candidate at INAOE, Mexico.</summary>
			<br />
			<img width="150" id="bio-image" src="/assets/images/speakers/speaker_horacio.jpeg" align="left" alt="headshot" hspace="10" />
            <p><a href="https://scholar.google.com/citations?user=zEl3pBEAAAAJ">Horacio Jarquin</a> is a PhD student at the National Institute of Astrophysics, Optics and Electronics, (INAOE). He holds the a bachelor degree from the Universidad Popular Autónoma del Estado de Puebla in 2018 a MSc degree from INAOE in 2020. His research interests are on machine learning and natural language processing, with emphasis in supervised classification; with particular interest on the development and application of deep learning approaches for vision and language representation.</p>
        </details>
        <hr />
    </details>
</div>

<script>
// JavaScript for Tabs
function openSection(evt, sectionName) {
  var i, tabcontent, tablinks;
  tabcontent = document.getElementsByClassName("tabcontent");
  for (i = 0; i < tabcontent.length; i++) {
    tabcontent[i].style.display = "none";
  }
  tablinks = document.getElementsByClassName("tablinks");
  for (i = 0; i < tablinks.length; i++) {
    tablinks[i].className = tablinks[i].className.replace(" active", "");
  }
  document.getElementById(sectionName).style.display = "block";
  evt.currentTarget.className += " active";
}

// Open the first tab by default
document.addEventListener("DOMContentLoaded", function() {
  document.getElementById("defaultOpen").click();
});
</script>

<!-- <button class="tablinks" onclick="openSection(event, 'Tutorials')" id="defaultOpen">Bio</button> -->



      

      

    </div>
  </div>
</div>


  <footer>
  <div class="container-md beautiful-jekyll-footer">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
      <ul class="list-inline text-center footer-links"><li class="list-inline-item">
    <a href="mailto:nlpschoolmx@gmail.com" title="Email me">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Email me</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://github.com/ampln" title="GitHub">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">GitHub</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://twitter.com/AMPLN" title="Twitter">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Twitter</span>
   </a>
  </li></ul>

      
      <p class="copyright text-muted">
      
        AMPLN
        &nbsp;&bull;&nbsp;
      
      2024

      

      
      </p>
      <p class="theme-by text-muted">
        Powered by
        <a href="https://beautifuljekyll.com">Beautiful Jekyll</a>
      </p>
      </div>
    </div>
  </div>
</footer>


  
  
    
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=" crossorigin="anonymous"></script>


  
    
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


  
    
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


  



  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script src="/assets/js/beautifuljekyll.js"></script>
    
  









</body>
</html>
